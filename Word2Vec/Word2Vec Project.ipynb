{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/jennyskytta/opt/anaconda3/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/jennyskytta/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [      # Very first step is to create the vocabulary \n",
    "    'he is a king',  # list of unique words with assigned indices \n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',  #in practice would perform normalization\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'a', 'king'],\n",
       " ['she', 'is', 'a', 'queen'],\n",
       " ['he', 'is', 'a', 'man'],\n",
       " ['she', 'is', 'a', 'woman'],\n",
       " ['warsaw', 'is', 'poland', 'capital'],\n",
       " ['berlin', 'is', 'germany', 'capital'],\n",
       " ['paris', 'is', 'france', 'capital']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_corpus(corpus):  \n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "tokenized_corpus # this gives us our list of tokens\n",
    " #iterate over tokens in corpus, and generate list of unique words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': 0, 'is': 1, 'a': 2, 'king': 3, 'she': 4, 'queen': 5, 'man': 6, 'woman': 7, 'warsaw': 8, 'poland': 9, 'capital': 10, 'berlin': 11, 'germany': 12, 'paris': 13, 'france': 14}\n",
      "{0: 'he', 1: 'is', 2: 'a', 3: 'king', 4: 'she', 5: 'queen', 6: 'man', 7: 'woman', 8: 'warsaw', 9: 'poland', 10: 'capital', 11: 'berlin', 12: 'germany', 13: 'paris', 14: 'france'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)} #create two dictionaries for mapping between word \n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)} # and index\n",
    "\n",
    "vocabulary_size = len(vocabulary) # size of our vocab (i.e number of words)\n",
    "print(word2idx) # word\n",
    "print(idx2word) #index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "idx_pairs[2] # paired words by index in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 4.6402668952941895\n",
      "Loss at epo 10: 4.022881507873535\n",
      "Loss at epo 20: 3.687854528427124\n",
      "Loss at epo 30: 3.475494861602783\n",
      "Loss at epo 40: 3.309539556503296\n",
      "Loss at epo 50: 3.1698966026306152\n",
      "Loss at epo 60: 3.0497453212738037\n",
      "Loss at epo 70: 2.945136308670044\n",
      "Loss at epo 80: 2.8530917167663574\n",
      "Loss at epo 90: 2.771200656890869\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.data\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(v,u):\n",
    "    return torch.dot(v,u)/(torch.norm(v)*torch.norm(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Question 1:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5850, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"she\"]], W2[word2idx[\"king\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9652, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"she\"]], W2[word2idx[\"queen\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Which pair is more similar? Does the model match your expectations?</b>\n",
    "\n",
    "The pair of \"she\" and \"queen\" are more similar at .96 which intuitively makes sense as we know that Queen is a gendered term appropriate to the use of \"she\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Question 2:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1659, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"warsaw\"]], W2[word2idx[\"poland\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2215, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"warsaw\"]], W2[word2idx[\"germany\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Which pair is more similar? Does the model match your expectations?</b>\n",
    "\n",
    "In this case, Warsaw and Poland are not similar which does not make much sense considering that Warsaw is the capitol city of Poland.  Warsaw and Germany however show greater but still low similiarity.  That would make sense independent of the previous comparison with Poland. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 3:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5439, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"warsaw\"]], W2[word2idx[\"capital\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3610, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"poland\"]], W2[word2idx[\"capital\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Which pair is more similar? Does the model match your expectations?</b>\n",
    "\n",
    "The over 50% similarity makes sense for Warsaw and Capital given that Warsaw is a capital city.  Poland and capital makes sense as well because you would expect the word Capital to be before Poland.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 4:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 4.652128219604492\n",
      "Loss at epo 10: 4.318161964416504\n",
      "Loss at epo 20: 4.055862903594971\n",
      "Loss at epo 30: 3.8413636684417725\n",
      "Loss at epo 40: 3.6616740226745605\n",
      "Loss at epo 50: 3.5086333751678467\n",
      "Loss at epo 60: 3.3767175674438477\n",
      "Loss at epo 70: 3.262006998062134\n",
      "Loss at epo 80: 3.1616220474243164\n",
      "Loss at epo 90: 3.07336163520813\n",
      "Loss at epo 100: 2.995483160018921\n",
      "Loss at epo 110: 2.926537036895752\n",
      "Loss at epo 120: 2.8652613162994385\n",
      "Loss at epo 130: 2.8105297088623047\n",
      "Loss at epo 140: 2.7613260746002197\n",
      "Loss at epo 150: 2.7167537212371826\n",
      "Loss at epo 160: 2.6760380268096924\n",
      "Loss at epo 170: 2.6385273933410645\n",
      "Loss at epo 180: 2.6036932468414307\n",
      "Loss at epo 190: 2.5711100101470947\n",
      "Loss at epo 200: 2.5404438972473145\n",
      "Loss at epo 0: 4.711775779724121\n",
      "Loss at epo 10: 4.165737628936768\n",
      "Loss at epo 20: 3.8739123344421387\n",
      "Loss at epo 30: 3.6618266105651855\n",
      "Loss at epo 40: 3.4994640350341797\n",
      "Loss at epo 50: 3.370413064956665\n",
      "Loss at epo 60: 3.264141321182251\n",
      "Loss at epo 70: 3.173863410949707\n",
      "Loss at epo 80: 3.0951850414276123\n",
      "Loss at epo 90: 3.0252134799957275\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 8\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = \n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.data\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0829, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"warsaw\"]], W2[word2idx[\"poland\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2058, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"warsaw\"]], W2[word2idx[\"germany\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0290, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"warsaw\"]], W2[word2idx[\"capital\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0290, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"warsaw\"]], W2[word2idx[\"capital\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code doesn't appear to be doing better if I'm interpretting correctly.  Now Warsaw and Poland are .08 and Warsaw and Capital are .02 which both seem inherently wrong.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Question 5:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jenny': 0, 'is': 1, 'the': 2, 'best': 3, 'she': 4, 'a': 5, 'queen': 6, 'he': 7, 'man': 8, 'woman': 9, 'washington': 10, 'state': 11, 'seattle': 12, 'capital': 13, 'city': 14, 'portland': 15, 'oregon': 16, 'paris': 17, 'france': 18}\n",
      "{0: 'jenny', 1: 'is', 2: 'the', 3: 'best', 4: 'she', 5: 'a', 6: 'queen', 7: 'he', 8: 'man', 9: 'woman', 10: 'washington', 11: 'state', 12: 'seattle', 13: 'capital', 14: 'city', 15: 'portland', 16: 'oregon', 17: 'paris', 18: 'france'}\n",
      "Loss at epo 0: 6.251488208770752\n",
      "Loss at epo 10: 5.0549845695495605\n",
      "Loss at epo 20: 4.600773334503174\n",
      "Loss at epo 30: 4.28962516784668\n",
      "Loss at epo 40: 4.028888702392578\n",
      "Loss at epo 50: 3.8029868602752686\n",
      "Loss at epo 60: 3.6071372032165527\n",
      "Loss at epo 70: 3.4392237663269043\n",
      "Loss at epo 80: 3.2968063354492188\n",
      "Loss at epo 90: 3.176079511642456\n",
      "Loss at epo 100: 3.0726959705352783\n",
      "Loss at epo 110: 2.9828290939331055\n",
      "Loss at epo 120: 2.9034786224365234\n",
      "Loss at epo 130: 2.8323872089385986\n",
      "Loss at epo 140: 2.7678864002227783\n",
      "Loss at epo 150: 2.70873761177063\n",
      "Loss at epo 160: 2.654024124145508\n",
      "Loss at epo 170: 2.603055953979492\n",
      "Loss at epo 180: 2.5553085803985596\n",
      "Loss at epo 190: 2.510378837585449\n",
      "Loss at epo 200: 2.467952251434326\n"
     ]
    }
   ],
   "source": [
    "corpus = [      # Very first step is to create the vocabulary \n",
    "    'skytta is the best',  # list of unique words with assigned indices \n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'washington is state',\n",
    "    'seattle is capital city',  #in practice would perform normalization\n",
    "    'portland is oregon capital',\n",
    "    'paris is france capital',   \n",
    "]\n",
    "\n",
    "def tokenize_corpus(corpus):  \n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "tokenized_corpus # this gives us our list of tokens\n",
    " #iterate over tokens in corpus, and generate list of unique words(tokens)\n",
    "    \n",
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)} #create two dictionaries for mapping between word \n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)} # and index\n",
    "\n",
    "vocabulary_size = len(vocabulary) # size of our vocab (i.e number of words)\n",
    "print(word2idx) # word\n",
    "print(idx2word) #index\n",
    "\n",
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "idx_pairs[2] # paired words by index in array\n",
    "\n",
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "embedding_dims = 10\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 201\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.data\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1286, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"jenny\"]], W2[word2idx[\"is\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7810, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"seattle\"]], W2[word2idx[\"state\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1094, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(W2[word2idx[\"seattle\"]], W2[word2idx[\"capital\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
