{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d8216c",
   "metadata": {},
   "source": [
    "## Python AI Chatbot\n",
    "This is a chatbot running in python 3.6 virutal environment.  The initial step will be to load that virtual environment. The code for this is provided by https://www.techwithtim.net/tutorials/ai-chatbot/chat-bot-part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37bfab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (3.6.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: joblib in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: tqdm in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from click->nltk) (4.8.3)\n",
      "Requirement already satisfied: zipp>=0.5 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.7.4.3)\n",
      "Requirement already satisfied: importlib-resources in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from tqdm->nltk) (5.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb2934d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.13 |Anaconda, Inc.| (default, Feb 23 2021, 12:58:59) \n",
      "[GCC Clang 10.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f54a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: virtualenv in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (20.17.1)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from virtualenv) (2.4.0)\n",
      "Requirement already satisfied: importlib-resources>=5.4 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from virtualenv) (5.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from virtualenv) (4.8.3)\n",
      "Requirement already satisfied: filelock<4,>=3.4.1 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from virtualenv) (3.4.1)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from virtualenv) (0.3.7)\n",
      "Requirement already satisfied: zipp>=0.5 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from importlib-metadata>=4.8.3->virtualenv) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from importlib-metadata>=4.8.3->virtualenv) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b04c27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflearn in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: six in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from tflearn) (1.15.0)\n",
      "Requirement already satisfied: Pillow in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from tflearn) (8.4.0)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from tflearn) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c4f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.2-cp36-cp36m-macosx_10_11_x86_64.whl (198.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 198.9 MB 35.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.7,>=2.6.0\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 29.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 51.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 27.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp36-cp36m-macosx_10_9_x86_64.whl (979 kB)\n",
      "\u001b[K     |████████████████████████████████| 979 kB 49.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.48.2-cp36-cp36m-macosx_10_10_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from tensorflow) (0.37.1)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras<2.7,>=2.6.0\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 47.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 41.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (58.0.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "\u001b[K     |████████████████████████████████| 289 kB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.8.3)\n",
      "Requirement already satisfied: zipp>=0.5 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 6.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2021.5.30)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 257 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[K     |████████████████████████████████| 143 kB 42.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 43.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: clang, termcolor, wrapt\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30694 sha256=5f1d9ebd2027ce0f630ee82f56826ee6ea29b4eca5a429b43883db0d2d276de5\n",
      "  Stored in directory: /Users/jennyskytta/Library/Caches/pip/wheels/22/4c/94/0583f60c9c5b6024ed64f290cb2d43b06bb4f75577dc3c93a7\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=8bd5ac2099b8fcc02a768df72d7cafb31e4fda0302ed07ff1eec59480bea0cfb\n",
      "  Stored in directory: /Users/jennyskytta/Library/Caches/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-macosx_10_9_x86_64.whl size=32619 sha256=8fcdc71b529432daf80e8cb98509543a8de017b8e6f88b9965af0010139ee8a1\n",
      "  Stored in directory: /Users/jennyskytta/Library/Caches/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built clang termcolor wrapt\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, typing-extensions, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, dataclasses, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, cached-property, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "Successfully installed absl-py-0.15.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.4 charset-normalizer-2.0.12 clang-5.0 dataclasses-0.8 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.48.2 h5py-3.1.0 idna-3.4 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.7 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.9 six-1.15.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.6.2 tensorflow-estimator-2.6.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.16 werkzeug-2.0.3 wrapt-1.12.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d79c101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jennyskytta\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b94247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jennyskytta/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Scipy not supported!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "with open('/Users/jennyskytta/Downloads/json/intents.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d46a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a40aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "        \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aebb1f",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "We are now going to preprocess our data.  We loaded in the JSON file.  We are creating a new list.  For each pattern, we want to add a new element for what intent \"tag\" its apart of for this chatbot.  So for each entry in docs_x corresponds to each entry in docs_y and the intent will be saved there.  This will be important for training our model.  \n",
    "\n",
    "## Stemming \n",
    "we want to stem all of our words in the words list and remove the duplicates to know the vocabulary size.  We will convert to lowercase and remove those dupes using 'Set'. We then sort everything. Now we want to setup our training and testing output.  \n",
    "\n",
    "## Bag of Words \n",
    "Neural networks only understand numbers rather than strings so we'll use \"bag of words\" or \"onehotencoded\" and this will represent the length encoding of the entries for the positions in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a6131f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]  # convert to lowercase \n",
    "words = sorted(list(set(words)))  # removes duplicates - takes all words - list converts back into list and then sort\n",
    "\n",
    "labels = sorted(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39521a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []   # Training is a blank list \n",
    "output = []   # output is a blank list \n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))] \n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af55d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = numpy.array(training)\n",
    "output = numpy.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c53b06",
   "metadata": {},
   "source": [
    "## The AI Model\n",
    "\n",
    "The model has the input layer that is looking at the number of words, next it goes into our two layers with 8 nodes.  Each word connects to a node in the hidden layers.  Finally, Softmax activiation calculates the probabilities for tags and predicting where to draw the response.  Finally, DNN is a type of neural network. The number of epochs is the number of times we're showing the model this data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31de106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.26309\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1000 | loss: 0.26309 - acc: 0.9759 -- iter: 18/26\n",
      "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.24500\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1000 | loss: 0.24500 - acc: 0.9783 -- iter: 26/26\n",
      "--\n",
      "INFO:tensorflow:/Users/jennyskytta/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "tensorflow.compat.v1.reset_default_graph()  #updated from training as previous was deprecated \n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)  # 8 neurons for the hidden layer\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")  #output layer\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "model.fit(training, output, n_epoch=1000, batch_size=9, show_metric=True)\n",
    "model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07da66",
   "metadata": {},
   "source": [
    "## Final Script\n",
    "\n",
    "here is the final full script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358dbc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/jennyskytta/model.tflearn\n",
      "Chat with Skyttabot (type quit to stop)!\n",
      "You: test\n",
      "I'm sorry, what on earth are you trying to convey - be clear- I'm a bot!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "with open('/Users/jennyskytta/Downloads/json/intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:    #rb = read bytes \n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = []   # these variables will be saved in pickle file \n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "tensorflow.compat.v1.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "try:\n",
    "    model.load(\"model.tflearn\")\n",
    "except:\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "    model.save(\"model.tflearn\")\n",
    "    \n",
    "\n",
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "\n",
    "def chat():\n",
    "    print(\"Chat with Skyttabot (type quit to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)])[0]\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "        \n",
    "        if results[results_index] > 0.7:\n",
    "            for tg in data[\"intents\"]:\n",
    "                if tg['tag'] == tag:\n",
    "                    responses = tg['responses']\n",
    "\n",
    "            print(random.choice(responses))\n",
    "        else:\n",
    "            print(\"I'm sorry, what on earth are you trying to convey - be clear- I'm a bot!\")\n",
    "\n",
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
